# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

"""
åŒæµYOLOæ£€æµ‹è®­ç»ƒå™¨

æ‰©å±•æ ‡å‡†çš„æ£€æµ‹è®­ç»ƒå™¨ä»¥æ”¯æŒåŒæµï¼ˆRGB + IRï¼‰è¾“å…¥çš„è®­ç»ƒ
"""

from __future__ import annotations

import math
import random
from copy import copy
from pathlib import Path
from typing import Any

import numpy as np
import torch
import torch.nn as nn

from ultralytics.data import build_dataloader
from ultralytics.data.build import build_dual_stream_dataset
from ultralytics.engine.trainer import BaseTrainer
from ultralytics.models import yolo
from ultralytics.nn.dual_tasks import DualStreamDetectionModel
from ultralytics.utils import DEFAULT_CFG, LOGGER, RANK
from ultralytics.utils.patches import override_configs
from ultralytics.utils.plotting import plot_images, plot_labels
from ultralytics.utils.torch_utils import torch_distributed_zero_first, unwrap_model


class DualStreamDetectionTrainer(BaseTrainer):
    """
    åŒæµYOLOæ£€æµ‹è®­ç»ƒå™¨

    è¯¥è®­ç»ƒå™¨ä¸“é—¨ç”¨äºè®­ç»ƒåŒæµYOLOæ¨¡å‹ï¼Œå¤„ç†RGBå’ŒIRå›¾åƒçš„åŒæ¨¡æ€è¾“å…¥ã€‚
    æ”¯æŒæ ‡å‡†YOLOè®­ç»ƒçš„æ‰€æœ‰åŠŸèƒ½ï¼ŒåŒæ—¶æ·»åŠ äº†åŒæµç‰¹å®šçš„æ•°æ®å¤„ç†å’Œæ¨¡å‹ç®¡ç†ã€‚

    ä¸»è¦ç‰¹ç‚¹ï¼š
    - æ”¯æŒRGB+IRåŒæµæ•°æ®åŠ è½½
    - è‡ªåŠ¨å¤„ç†6é€šé“è¾“å…¥çš„é¢„å¤„ç†
    - å…¼å®¹æ ‡å‡†YOLOè®­ç»ƒæµç¨‹
    - æ”¯æŒåŒæµç‰¹å®šçš„å¯è§†åŒ–

    Attributes:
        model (DualStreamDetectionModel): åŒæµæ£€æµ‹æ¨¡å‹
        data (dict): åŒ…å«RGBå’ŒIRæ•°æ®è·¯å¾„çš„æ•°æ®é›†é…ç½®
        loss_names (tuple): æŸå¤±ç»„ä»¶åç§°

    Examples:
        >>> from ultralytics.models.yolo.detect.dual_train import DualStreamDetectionTrainer
        >>> args = dict(model="yolo26n.pt", data="dual_dataset.yaml", epochs=100)
        >>> trainer = DualStreamDetectionTrainer(overrides=args)
        >>> trainer.train()
    """

    def __init__(self, cfg=DEFAULT_CFG, overrides: dict[str, Any] | None = None, _callbacks=None):
        """
        åˆå§‹åŒ–åŒæµæ£€æµ‹è®­ç»ƒå™¨

        Args:
            cfg (dict): é»˜è®¤é…ç½®å­—å…¸
            overrides (dict): å‚æ•°è¦†ç›–å­—å…¸
            _callbacks (list): å›è°ƒå‡½æ•°åˆ—è¡¨
        """
        super().__init__(cfg, overrides, _callbacks)

    def build_dataset(self, img_path: str, mode: str = "train", batch: int | None = None):
        """
        æ„å»ºåŒæµYOLOæ•°æ®é›†

        Args:
            img_path (str): å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„ï¼ˆå¯¹äºåŒæµï¼Œè¿™ä¸ªå‚æ•°ä¼šè¢«dataé…ç½®ä¸­çš„è·¯å¾„è¦†ç›–ï¼‰
            mode (str): è®­ç»ƒæ¨¡å¼ ('train' æˆ– 'val')
            batch (int): æ‰¹æ¬¡å¤§å°

        Returns:
            Dataset: é…ç½®å¥½çš„åŒæµæ•°æ®é›†
        """
        gs = max(int(unwrap_model(self.model).stride.max() if self.model else 0), 32)
        data_root = Path(self.data.get("path")) if self.data and self.data.get("path") else None

        def _resolve_path(value):
            if value is None:
                return value
            if isinstance(value, (list, tuple)):
                return [_resolve_path(v) for v in value]
            p = Path(value)
            if p.is_absolute() or data_root is None:
                return str(p)
            candidate = (data_root / p).resolve()
            if not candidate.exists() and str(value).startswith("../"):
                candidate = (data_root / str(value)[3:]).resolve()
            return str(candidate)

        # æ£€æŸ¥æ˜¯å¦æœ‰åŒæµæ•°æ®è·¯å¾„é…ç½®
        if 'rgb_' + mode in self.data and 'ir_' + mode in self.data:
            # åŒæµæ¨¡å¼ï¼šåˆ†åˆ«æŒ‡å®šRGBå’ŒIRè·¯å¾„
            rgb_key = f"rgb_{mode}"
            ir_key = f"ir_{mode}"
            labels_key = f"labels_{mode}"
            rgb_path = _resolve_path(self.data[rgb_key])
            ir_path = _resolve_path(self.data[ir_key])
            self.data[rgb_key] = rgb_path
            self.data[ir_key] = ir_path
            if self.data.get(labels_key):
                self.data[labels_key] = _resolve_path(self.data[labels_key])

            LOGGER.info(f"æ„å»ºåŒæµæ•°æ®é›† - RGB: {rgb_path}, IR: {ir_path}")

            return build_dual_stream_dataset(
                self.args,
                rgb_path,
                ir_path,
                batch,
                self.data,
                mode=mode,
                rect=mode == "val",
                stride=gs
            )
        else:
            # å¦‚æœæ²¡æœ‰åŒæµé…ç½®ï¼Œå°è¯•ä»æ ‡å‡†è·¯å¾„æ¨æ–­
            LOGGER.warning("æœªæ‰¾åˆ°åŒæµæ•°æ®é…ç½® (rgb_train, ir_train ç­‰)ï¼Œå°è¯•ä»æ ‡å‡†è·¯å¾„æ¨æ–­...")

            # å‡è®¾æ ‡å‡†è·¯å¾„ä¸‹æœ‰rgbå’Œirå­æ–‡ä»¶å¤¹
            base_path = Path(img_path)
            rgb_path = base_path / 'rgb'
            ir_path = base_path / 'ir'

            if rgb_path.exists() and ir_path.exists():
                LOGGER.info(f"è‡ªåŠ¨æ£€æµ‹åˆ°åŒæµæ•°æ® - RGB: {rgb_path}, IR: {ir_path}")
                return build_dual_stream_dataset(
                    self.args,
                    str(rgb_path),
                    str(ir_path),
                    batch,
                    self.data,
                    mode=mode,
                    rect=mode == "val",
                    stride=gs
                )
            else:
                raise ValueError(
                    f"åŒæµæ•°æ®é…ç½®é”™è¯¯ï¼è¯·åœ¨æ•°æ®é…ç½®æ–‡ä»¶ä¸­æŒ‡å®š:\n"
                    f"rgb_{mode}: /path/to/rgb/images\n"
                    f"ir_{mode}: /path/to/ir/images\n"
                    f"æˆ–ç¡®ä¿ {img_path} ä¸‹å­˜åœ¨ 'rgb' å’Œ 'ir' å­æ–‡ä»¶å¤¹"
                )

    def get_dataloader(self, dataset_path: str, batch_size: int = 16, rank: int = 0, mode: str = "train"):
        """
        æ„å»ºåŒæµæ•°æ®åŠ è½½å™¨

        Args:
            dataset_path (str): æ•°æ®é›†è·¯å¾„
            batch_size (int): æ‰¹æ¬¡å¤§å°
            rank (int): åˆ†å¸ƒå¼è®­ç»ƒè¿›ç¨‹rank
            mode (str): æ¨¡å¼ ('train' æˆ– 'val')

        Returns:
            DataLoader: PyTorchæ•°æ®åŠ è½½å™¨
        """
        assert mode in {"train", "val"}, f"æ¨¡å¼å¿…é¡»æ˜¯ 'train' æˆ– 'val'ï¼Œè€Œä¸æ˜¯ {mode}."

        with torch_distributed_zero_first(rank):
            dataset = self.build_dataset(dataset_path, mode, batch_size)

        shuffle = mode == "train"
        if getattr(dataset, "rect", False) and shuffle:
            LOGGER.warning("'rect=True' ä¸DataLoader shuffleä¸å…¼å®¹ï¼Œè®¾ç½®shuffle=False")
            shuffle = False

        return build_dataloader(
            dataset,
            batch=batch_size,
            workers=self.args.workers if mode == "train" else self.args.workers * 2,
            shuffle=shuffle,
            rank=rank,
            drop_last=self.args.compile and mode == "train",
        )

    def preprocess_batch(self, batch: dict) -> dict:
        """
        é¢„å¤„ç†æ‰¹æ¬¡æ•°æ®ï¼Œå¤„ç†6é€šé“è¾“å…¥

        Args:
            batch (dict): åŒ…å«å›¾åƒå’Œæ ‡ç­¾çš„æ‰¹æ¬¡å­—å…¸

        Returns:
            dict: é¢„å¤„ç†åçš„æ‰¹æ¬¡æ•°æ®
        """
        # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
        for k, v in batch.items():
            if isinstance(v, torch.Tensor):
                batch[k] = v.to(self.device, non_blocking=self.device.type == "cuda")

        # å½’ä¸€åŒ–å›¾åƒ (0-255 -> 0-1)
        batch["img"] = batch["img"].float() / 255

        # éªŒè¯è¾“å…¥ç»´åº¦
        imgs = batch["img"]
        if imgs.shape[1] != 6:
            raise ValueError(f"åŒæµæ¨¡å‹æœŸæœ›6é€šé“è¾“å…¥ï¼Œä½†æ”¶åˆ°{imgs.shape[1]}é€šé“")

        # å¤šå°ºåº¦è®­ç»ƒ
        multi_scale = self.args.multi_scale
        if random.random() < multi_scale:
            sz = (
                random.randrange(int(self.args.imgsz * 0.5), int(self.args.imgsz * 1 + self.stride))
                // self.stride
                * self.stride
            )
            sf = sz / max(imgs.shape[2:])
            if sf != 1:
                ns = [
                    math.ceil(x * sf / self.stride) * self.stride for x in imgs.shape[2:]
                ]
                imgs = nn.functional.interpolate(imgs, size=ns, mode="bilinear", align_corners=False)
            batch["img"] = imgs

        return batch

    def set_model_attributes(self):
        """æ ¹æ®æ•°æ®é›†ä¿¡æ¯è®¾ç½®æ¨¡å‹å±æ€§"""
        self.model.nc = self.data["nc"]
        self.model.names = self.data["names"]
        self.model.args = self.args

        # ç¡®ä¿æ¨¡å‹çŸ¥é“å®ƒæ˜¯åŒæµæ¨¡å¼
        if hasattr(self.model, 'is_dual_stream'):
            LOGGER.info("æ¨¡å‹å·²é…ç½®ä¸ºåŒæµæ¨¡å¼")
        else:
            LOGGER.warning("æ¨¡å‹å¯èƒ½ä¸æ”¯æŒåŒæµï¼Œä½†å°†å°è¯•ä½¿ç”¨6é€šé“è¾“å…¥")

    def get_model(self, cfg: str | None = None, weights: str | None = None, verbose: bool = True):
        """
        è·å–åŒæµæ£€æµ‹æ¨¡å‹

        Args:
            cfg (str): æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„
            weights (str): æ¨¡å‹æƒé‡è·¯å¾„
            verbose (bool): æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯

        Returns:
            DualStreamDetectionModel: åŒæµæ£€æµ‹æ¨¡å‹
        """
        # å¼ºåˆ¶è®¾ç½®ä¸º6é€šé“è¾“å…¥
        model = DualStreamDetectionModel(
            cfg,
            nc=self.data["nc"],
            ch=6,  # RGB 3 + IR 3
            verbose=verbose and RANK == -1
        )

        if weights:
            model.load(weights)

        # è®¾ç½®æ•°æ®é›†é€šé“æ•°
        self.data["channels"] = 6

        return model

    def get_validator(self):
        """è·å–åŒæµæ£€æµ‹éªŒè¯å™¨"""
        self.loss_names = "box_loss", "cls_loss", "dfl_loss"
        return yolo.detect.DetectionValidator(
            self.test_loader,
            save_dir=self.save_dir,
            args=copy(self.args),
            _callbacks=self.callbacks
        )

    def plot_training_samples(self, batch: dict[str, Any], ni: int) -> None:
        """
        ç»˜åˆ¶è®­ç»ƒæ ·æœ¬ï¼Œåˆ†åˆ«æ˜¾ç¤ºRGBå’ŒIRå›¾åƒ

        Args:
            batch (dict): æ‰¹æ¬¡æ•°æ®
            ni (int): è¿­ä»£æ¬¡æ•°
        """
        imgs = batch["img"]

        # åˆ†ç¦»RGBå’ŒIRå›¾åƒç”¨äºå¯è§†åŒ–
        rgb_imgs = imgs[:, :3, :, :] * 255  # RGBå‰3é€šé“
        ir_imgs = imgs[:, 3:, :, :] * 255   # IRå3é€šé“

        # åˆ›å»ºRGBå¯è§†åŒ–
        rgb_batch = batch.copy()
        rgb_batch["img"] = rgb_imgs
        plot_images(
            labels=rgb_batch,
            paths=batch["im_file"],
            fname=self.save_dir / f"train_batch{ni}_rgb.jpg",
            on_plot=self.on_plot,
        )

        # åˆ›å»ºIRå¯è§†åŒ–ï¼ˆè½¬æ¢ä¸ºä¼ªå½©è‰²æ˜¾ç¤ºï¼‰
        ir_batch = batch.copy()
        ir_batch["img"] = ir_imgs
        plot_images(
            labels=ir_batch,
            paths=batch["im_file"],
            fname=self.save_dir / f"train_batch{ni}_ir.jpg",
            on_plot=self.on_plot,
        )

    def plot_training_labels(self):
        """åˆ›å»ºè®­ç»ƒæ ‡ç­¾åˆ†å¸ƒå›¾"""
        boxes = np.concatenate([lb["bboxes"] for lb in self.train_loader.dataset.labels], 0)
        cls = np.concatenate([lb["cls"] for lb in self.train_loader.dataset.labels], 0)
        plot_labels(boxes, cls.squeeze(), names=self.data["names"], save_dir=self.save_dir, on_plot=self.on_plot)

    def auto_batch(self):
        """
        è‡ªåŠ¨è®¡ç®—æœ€ä¼˜æ‰¹æ¬¡å¤§å°

        Returns:
            int: æœ€ä¼˜æ‰¹æ¬¡å¤§å°
        """
        with override_configs(self.args, overrides={"cache": False}) as self.args:
            train_dataset = self.build_dataset(self.data["train"], mode="train", batch=16)
        max_num_obj = max(len(label["cls"]) for label in train_dataset.labels) * 4
        del train_dataset
        return super().auto_batch(max_num_obj)

    def validate_data_config(self):
        """éªŒè¯åŒæµæ•°æ®é…ç½®"""
        required_keys = {
            "train": ["rgb_train", "ir_train"],
            "val": ["rgb_val", "ir_val"]
        }

        for split, keys in required_keys.items():
            for key in keys:
                if key not in self.data:
                    raise ValueError(
                        f"åŒæµæ•°æ®é…ç½®ç¼ºå°‘ '{key}' é”®ã€‚\n"
                        f"è¯·ç¡®ä¿æ•°æ®é…ç½®æ–‡ä»¶åŒ…å«ä»¥ä¸‹é”®ï¼š\n"
                        f"rgb_train: /path/to/rgb/train/images\n"
                        f"ir_train: /path/to/ir/train/images\n"
                        f"rgb_val: /path/to/rgb/val/images\n"
                        f"ir_val: /path/to/ir/val/images"
                    )

    def train(self):
        """è¿è¡ŒåŒæµè®­ç»ƒ"""
        # éªŒè¯æ•°æ®é…ç½®
        try:
            self.validate_data_config()
        except ValueError as e:
            LOGGER.warning(f"æ•°æ®é…ç½®éªŒè¯è­¦å‘Š: {e}")

        # è°ƒç”¨çˆ¶ç±»è®­ç»ƒæ–¹æ³•
        return super().train()
